{"cells":[{"cell_type":"code","execution_count":88,"metadata":{"executionInfo":{"elapsed":787,"status":"ok","timestamp":1723625787831,"user":{"displayName":"Hariharan Ganapathy","userId":"18052769655460982476"},"user_tz":-330},"id":"Odxls14cuLXi"},"outputs":[],"source":["import pandas as pd\n","from pandas import DataFrame\n","import numpy as np\n","import json\n","import os\n","\n","def __get_abs_fpaths__(dir):\n","    for dirpath,_,filenames in os.walk(dir):\n","        for f in filenames:\n","            yield os.path.abspath(os.path.join(dirpath, f))\n","\n","class Base:\n","  def __init__(self, record_type, columns,data_dir, top_columns=None, hover_columns=None):\n","    self.__record_type__= record_type\n","    self.__columns__= columns\n","    if top_columns is not None:\n","      self.__top_columns__ = top_columns\n","    else:\n","      self.__top_columns__ = columns\n","\n","    if hover_columns is not None:\n","      self.__hover_columns__ = hover_columns\n","    else:\n","      self.__hover_columns__ = columns\n","\n","    self.__aggregated_dir__= f'''{data_dir}/aggregated/{record_type}/'''\n","    self.__top_dir__=f'''{data_dir}/top/{record_type}/'''\n","    self.__hover_dir__= f'''{data_dir}/map/{record_type}/hover'''\n","    self.__record_class__ = ['aggregated','top','hover']\n","    self.__aggregated_result__ = None\n","    self.__top_result__ = None\n","    self.__hover_result__ = None\n","\n","  '''\n","    Load a JSON data file into a dataframe,given the full path\n","  '''\n","  def __load_file__ (f):\n","    try:\n","      df = pd.read_json(f)\n","      df.drop(columns=['success','code'])\n","      return df\n","    except:\n","      raise Exception(f'''File not found: {f}''')\n","\n","  '''\n","    Load data\n","  '''\n","  def load(self):\n","    #print( '''load() - Start''')\n","\n","    # Load aggregated\n","    result=self.load_aggregated()\n","\n","    # Load top\n","    result = self.load_top()\n","\n","    # Load map hover data\n","    #result=self.load_hover()\n","\n","    #print( f'''load_data() {count} files processed - End''')\n","\n","  '''\n","    Load aggregate data\n","  '''\n","  def load_aggregated(self):\n","    result = dict()\n","    for c in self.__columns__:\n","      result[c]=list()\n","\n","    for f in __get_abs_fpaths__(self.__aggregated_dir__):\n","      self.__decode_aggregate__(f,result)\n","\n","    self.__aggregated_result__ = pd.DataFrame(result, columns= self.__columns__ )\n","\n","  '''\n","    Load top data\n","  '''\n","  def load_top(self):\n","    result = dict()\n","    for c in self.__top_columns__:\n","      result[c]=list()\n","\n","    for f in __get_abs_fpaths__(self.__top_dir__):\n","      self.__decode_top__(f,result)\n","\n","    self.__top_result__ = pd.DataFrame(result, columns=self.__top_columns__ )\n","  '''\n","    Load hover data\n","  '''\n","  def load_hover(self):\n","    result = dict()\n","    for c in self.__hover_columns__:\n","      result[c]=list()\n","    for f in __get_abs_fpaths__(self.__hover_dir__):\n","      self.__decode_hover__(f,result)\n","\n","    self.__hover_result__ = pd.DataFrame(result, columns=self.__hover_columns__ )\n","\n","\n","  '''\n","    Return a dataframe with aggregated data\n","  '''\n","  def aggregated(self):\n","    return self.__aggregated_result__\n","\n","  '''\n","    Return a dataframe with top data\n","  '''\n","  def top(self):\n","    return self.__top_result__\n","\n","  '''\n","    Return a dataframe with hover data\n","  '''\n","  def hover(self):\n","    return self.__hover_result__\n","\n","  '''\n","    Write the loaded results into CSV files\n","  '''\n","  def to_csv(self):\n","    if self.__aggregated_result__  is not None:\n","      self.__aggregated_result__.to_csv(f'{self.__record_type__}_agg.csv', sep=',', encoding='utf-8', index=False)\n","\n","    if self.__top_result__  is not None:\n","      self.__top_result__.to_csv(f'{self.__record_type__}_top.csv', sep=',', encoding='utf-8',index=False)\n","\n","    if self.__hover_result__ is not None:\n","      self.__hover_result__.to_csv(f'{self.__record_type__}_hover.csv', sep=',', encoding='utf-8',index=False)\n","\n","  '''\n","    Read CSV files in a given folder and load the dataframes\n","  '''\n","  def from_csv(self,folder):\n","    f=f'''{folder}/{self.__record_type__}_agg.csv'''\n","    if os.path.exists(f):\n","      self.__aggregated_result__ = pd.read_csv(f)\n","      self.__aggregated_result__.geo_name = self.__aggregated_result__.geo_name.astype(str)\n","\n","\n","    f=f'''{folder}/{self.__record_type__}_top.csv'''\n","    if os.path.exists(f):\n","      self.__top_result__ = pd.read_csv(f)\n","      self.__top_result__.geo_name = self.__top_result__.geo_name.astype(str)\n","\n","    f=f'''{folder}/{self.__record_type__}_hover.csv'''\n","    if os.path.exists(f):\n","      self.__hover_result__ = pd.read_csv(f)\n","      self.__hover_result__.geo_name = self.__hover_result__.geo_name.astype(str)\n","\n","\n","  def __lookup_geo_id__(geo_typename,geo):\n","    geo_type, geo_name, geo_pname = geo_typename.split(',')\n","\n","    if geo_type == 'STA':\n","      geo_name=geo_name.replace(' ','-')\n","\n","    #print(f'''geo_type:*{geo_type}*, geo_name:*{geo_name}*''')\n","    geo_recs = geo[(geo['geo_type']==geo_type) & (geo['name']==geo_name)].reset_index()\n","    if geo_recs.shape[0]==1:\n","      return geo_recs.loc[0,'id']\n","    elif geo_recs.shape[0] > 1:\n","      if geo_pname is not None:\n","        if geo_pname != 'nan':\n","          precs = geo[geo.name==geo_pname].reset_index()\n","          if precs.shape[0] > 0 :\n","            return geo_recs[geo_recs.parent_id == precs.loc[0,'id']].reset_index().loc[0,'id']\n","        else:\n","            return geo_recs.sort_values('parent_id', ascending=True).loc[0,'id']\n","    return -1\n","\n","  def __combine_geo_typename(geo_type,geo_name,geo_pname):\n","    if geo_pname is not None:\n","      return geo_type + ',' + geo_name + ',' + geo_pname\n","    else:\n","      return geo_type + ',' + geo_name\n","\n","  def __preprocess__(self,df,geo):\n","    df['geo_name']=df['geo_name'].astype(str)\n","    df['geo_pname']=df['geo_pname'].astype(str)\n","\n","\n","    # Lookup the geo_id from type and name\n","    df[\"geo_typename\"] = df.apply(lambda x: Base.__combine_geo_typename(x['geo_type'], x['geo_name'], x['geo_pname']), axis=1)\n","    df['geo_typename'].astype(str)\n","    df['geo_id']=df['geo_typename'].apply(Base.__lookup_geo_id__, args=(geo,))\n","\n","    # Drop unnecessary columns\n","    df.drop(columns=['geo_type', 'geo_name', 'geo_typename', 'geo_pname'], inplace=True)\n","    df = df.reset_index().rename(columns={'index':'id'})\n","    return df\n","\n","  '''\n","    Store the loaded records to DB\n","  '''\n","  def store(self, con, geo):\n","    self.__preprocess_and_store__(con, geo)\n","\n","  '''\n","    Decode the common fields\n","  '''\n","  def __decode_common__(f, result):\n","    #print('''decode_common() - Start ''')\n","    state=None\n","    year=None\n","    quarter=None\n","    if 'state' in f:\n","      state,year,quarter = os.path.normpath(f).split(os.sep)[-3:]\n","    else:\n","      year,quarter=os.path.normpath(f).split(os.sep)[-2:]\n","    quarter=quarter.split('.')[0]\n","    geo_type='CON'\n","    geo_name='india'\n","    geo_pname=None\n","    if year is not None and quarter is not None:\n","      if state is not None:\n","        geo_type='STA'\n","        geo_name=state\n","        geo_pname='india'\n","\n","    #print('''decode_common() - End ''')\n","    result['year'].append(year)\n","    result['quarter'].append(quarter)\n","    result['geo_type'].append(geo_type)\n","    result['geo_name'].append(geo_name)\n","    result['geo_pname'].append(geo_pname)\n","\n","\n","#-------------------------------- End of Base class ----------------------------------\n","\n","'''\n","  Class to handle loading and saving of 'transaction' data\n","'''\n","class Transaction (Base):\n","  def __init__(self, data_dir='/content/data'):\n","    Base.__init__(self,\n","                  'transaction',\n","                   ['year','quarter', 'geo_type', 'geo_name', 'geo_pname','category', 'stat_type', 'count', 'amount'],\n","                  data_dir,\n","                  top_columns=['year','quarter', 'geo_type', 'geo_name','geo_pname','top_in','stat_type', 'count', 'amount'],\n","                  hover_columns=['year','quarter', 'geo_type', 'geo_name','geo_pname','hover_over','stat_type', 'count', 'amount'])\n","\n","  '''\n","    Load all Transaction stats\n","  '''\n","  def __decode_aggregate__(self,f,result):\n","    #print('Transaction.__decode_aggregate__ - Start')\n","\n","    # Load JSON contents into a temp dataframe\n","    df=Base.__load_file__(f)\n","\n","    # Seek to the contents of the 'transaction' data\n","    txn_recs = df.loc['transactionData','data']\n","    for txn_rec in txn_recs:\n","      # Collect the common fields from path name\n","      Base.__decode_common__(f,result)\n","\n","      # Process record for all category\n","      for payment_rec in txn_rec['paymentInstruments']:\n","        result['category'].append(txn_rec[\"name\"])\n","        result['stat_type'].append(payment_rec[\"type\"])\n","        result['count'].append(payment_rec[\"count\"])\n","        result['amount'].append(payment_rec[\"amount\"])\n","\n","    #print('Transaction.__decode_aggregate__ - End')\n","\n","  def __decode_top__(self,f,result):\n","    # Load JSON contents into a temp dataframe\n","    df=Base.__load_file__(f)\n","\n","    # Seek to the contents of the 'transaction' data\n","    txn_states = df.loc['states','data']\n","    txn_districts = df.loc['districts','data']\n","    txn_pincodes = df.loc['pincodes','data']\n","\n","    if txn_states is not None:\n","      for txn in txn_states:\n","        Base.__decode_common__(f,result)\n","        result['top_in'].append(result['geo_type'][-1])\n","        result['geo_type'][-1]='STA'\n","        result['geo_pname'][-1]='india'\n","        result['geo_name'][-1]=txn['entityName']\n","        result['stat_type'].append(txn['metric'][\"type\"])\n","        result['count'].append(txn['metric'][\"count\"])\n","        result['amount'].append(txn['metric'][\"amount\"])\n","\n","    if txn_districts is not None:\n","      for txn in txn_districts:\n","        Base.__decode_common__(f,result)\n","        result['top_in'].append(result['geo_type'][-1])\n","        result['geo_type'][-1]='DIS'\n","        if result['top_in'][-1] == 'STA':\n","          result['geo_pname'][-1]=result['geo_name'][-1]\n","        result['geo_name'][-1]=txn['entityName']\n","        result['stat_type'].append(txn['metric'][\"type\"])\n","        result['count'].append(txn['metric'][\"count\"])\n","        result['amount'].append(txn['metric'][\"amount\"])\n","\n","    if txn_pincodes is not None:\n","      for txn in txn_pincodes:\n","        Base.__decode_common__(f,result)\n","        result['top_in'].append(result['geo_type'][-1])\n","        result['geo_type'][-1]='PIN'\n","        if result['top_in'][-1] == 'STA':\n","          result['geo_pname'][-1]=result['geo_name'][-1]\n","        result['geo_name'][-1]=txn['entityName']\n","        result['stat_type'].append(txn['metric'][\"type\"])\n","        result['count'].append(txn['metric'][\"count\"])\n","        result['amount'].append(txn['metric'][\"amount\"])\n","\n","  def __decode_hover__(self,f,result):\n","    # Load JSON contents into a temp dataframe\n","    df=Base.__load_file__(f)\n","    #print(df.to_markdown())\n","    txn_hover_recs = df.loc['hoverDataList','data']\n","    for txn_hover_rec in txn_hover_recs:\n","      # Collect the common fields from path name\n","      Base.__decode_common__(f,result)\n","      result['hover_over'].append(result['geo_type'][-1])\n","      result['geo_pname'][-1]=result['geo_name'][-1]\n","\n","      if result['hover_over'][-1] == 'CON':\n","        result['geo_type'][-1]='STA'\n","      else:\n","        result['geo_type'][-1]='DIS'\n","\n","      metric=txn_hover_rec['metric'][0]\n","      result['geo_name'][-1]=txn_hover_rec['name']\n","      if result['geo_type'][-1] == 'DIS':\n","        result['geo_name'][-1] = result['geo_name'][-1].split(' district')[0] # Remove  the ' district' at the end of the name\n","      result['stat_type'].append(metric['type'])\n","      result['count'].append(metric['count'])\n","      result['amount'].append(metric['amount'])\n","\n","\n","\n","  def __preprocess_and_store__(self, conn, geo):\n","    agg_txns = self.__preprocess__(self.__aggregated_result__, geo=geo)\n","    top_txns = self.__preprocess__(self.__top_result__, geo=geo)\n","    hover_txns=self.__preprocess__(self.__hover_result__, geo=geo)\n","    print(f'''agg_txns(db): {agg_txns.shape}, top_txns(db):{top_txns.shape}, hover_txns(db):{hover_txns.shape}''')\n","\n","    # Write to 'transaction_*' tables\n","    agg_txns.to_sql(con=conn, name='transaction_agg',if_exists='append',index=False)\n","    top_txns.to_sql(con=conn, name='transaction_top',if_exists='append',index=False)\n","    hover_txns.to_sql(con=conn, name='transaction_hover',if_exists='append',index=False)\n","    conn.commit()\n","\n","#---------------------------------------------End of Transaction class --------------------------\n","'''\n","  Class to load and save 'User' data\n","'''\n","class User(Base):\n","\n","  def __init__(self, data_dir=\"/content/data\"):\n","    Base.__init__(\n","        self,\n","        'user',\n","        ['year','quarter', 'geo_type', 'geo_name', 'geo_pname','reg_users', 'app_opens','brand','count','percentage'],\n","        data_dir,\n","        top_columns=['year','quarter', 'geo_type', 'geo_name','geo_pname','top_in', 'reg_users'],\n","        hover_columns=['year','quarter', 'geo_type', 'geo_name','geo_pname','hover_over', 'reg_users', 'app_opens'])\n","\n","  '''\n","    Decodes aggregate user data\n","  '''\n","  def __decode_aggregate__(self,f,result):\n","    #print('decode_user - Start')\n","\n","    # Load JSON contents into a temp dataframe\n","    df=Base.__load_file__(f)\n","\n","    # Seek to the contents of the 'user.aggregated' data\n","    user_stat_rec = df.loc['aggregated','data']\n","\n","    # Seek to the contents of the 'user.device' data\n","    device_recs = df.loc['usersByDevice','data']\n","\n","    # For each device row populate both aggregate and device fields\n","    if device_recs is not None:\n","      for device_rec in device_recs:\n","        # Collect the common fields from path name\n","        Base.__decode_common__(f,result)\n","        result['reg_users'].append(user_stat_rec['registeredUsers'])\n","        result['app_opens'].append(user_stat_rec['appOpens'])\n","        result['brand'].append(device_rec['brand'])\n","        result['count'].append(device_rec['count'])\n","        result['percentage'].append(device_rec['percentage'])\n","    else:\n","      Base.__decode_common__(f,result)\n","      result['reg_users'].append(user_stat_rec['registeredUsers'])\n","      result['app_opens'].append(user_stat_rec['appOpens'])\n","      result['brand'].append('Unknown')\n","      result['count'].append(0)\n","      result['percentage'].append(100)\n","\n","  #print('decode_user - End')\n","\n","  '''\n","    Decodes top user data\n","  '''\n","  def __decode_top__(self,f,result):\n","    # Load JSON contents into a temp dataframe\n","    df=Base.__load_file__(f)\n","\n","    # Seek to the contents of the 'transaction' data\n","    user_states = df.loc['states','data']\n","    user_districts = df.loc['districts','data']\n","    user_pincodes = df.loc['pincodes','data']\n","\n","\n","    if user_states is not None:\n","      for user in user_states:\n","        Base.__decode_common__(f,result)\n","        result['top_in'].append(result['geo_type'][-1])\n","        result['geo_type'][-1]='STA'\n","        result['geo_pname'][-1]='india'\n","        result['geo_name'][-1]=user['name']\n","        result['reg_users'].append(user['registeredUsers'])\n","\n","    if user_districts is not None:\n","      for user in user_districts:\n","        Base.__decode_common__(f,result)\n","        result['top_in'].append(result['geo_type'][-1])\n","        result['geo_type'][-1]='DIS'\n","        if result['top_in'][-1] == 'STA':\n","          result['geo_pname'][-1]=result['geo_name'][-1]\n","        result['geo_name'][-1]=user['name']\n","        result['reg_users'].append(user['registeredUsers'])\n","\n","    if user_pincodes is not None:\n","      for user in user_pincodes:\n","        Base.__decode_common__(f,result)\n","        result['top_in'].append(result['geo_type'][-1])\n","        result['geo_type'][-1]='PIN'\n","        if result['top_in'][-1] == 'STA':\n","          result['geo_pname'][-1]=result['geo_name'][-1]\n","        result['geo_name'][-1]=user['name']\n","        result['reg_users'].append(user['registeredUsers'])\n","\n","  def __decode_hover__(self,f,result):\n","    # Load JSON contents into a temp dataframe\n","    df=Base.__load_file__(f)\n","    #print(df.to_markdown())\n","    usr_hover_recs = df.loc['hoverData','data']\n","    for rec_key in usr_hover_recs.keys():\n","      usr_hover_rec = usr_hover_recs[rec_key]\n","      # Collect the common fields from path name\n","      Base.__decode_common__(f,result)\n","      result['hover_over'].append(result['geo_type'][-1])\n","      result['geo_pname'][-1]=result['geo_name'][-1]\n","\n","      if result['hover_over'][-1] == 'CON':\n","        result['geo_type'][-1]='STA'\n","      else:\n","        result['geo_type'][-1]='DIS'\n","\n","      result['geo_name'][-1]=rec_key\n","      if result['geo_type'][-1] == 'DIS':\n","        result['geo_name'][-1] = result['geo_name'][-1].split(' district')[0] # Remove  the ' district' at the end of the name\n","      result['reg_users'].append(usr_hover_rec['registeredUsers'])\n","      result['app_opens'].append(usr_hover_rec['appOpens'])\n","\n","  def __preprocess_and_store__(self, conn, geo):\n","    agg_users = self.__preprocess__(self.__aggregated_result__, geo=geo)\n","    top_users = self.__preprocess__(self.__top_result__, geo=geo)\n","    hover_users = self.__preprocess__(self.__hover_result__, geo=geo)\n","    print(f'''Read From CSV: agg_users(db): {agg_users.shape}, top_users(db):{top_users.shape}, hover_users(db):{hover_users.shape}''')\n","\n","    df_user_agg = agg_users[['year','quarter','geo_id','reg_users','app_opens']]\n","    df_device_agg = agg_users[['year','quarter','geo_id','brand','count','percentage']].reset_index().rename(columns={'index':'id'})\n","    df_user_agg.drop_duplicates(inplace=True,ignore_index=True)\n","\n","    df_user_agg = df_user_agg.reset_index().rename(columns={'index':'id'})\n","    df_user_agg['stat_type']='TOTAL'\n","    top_users['stat_type']='TOTAL'\n","    hover_users['stat_type']='TOTAL'\n","\n","\n","    # Write to 'user_**' tables\n","    df_user_agg.to_sql(con=conn, name='user_agg',if_exists='append',index=False)\n","    df_device_agg.to_sql(con=conn, name='device_agg',if_exists='append',index=False)\n","    top_users.to_sql(con=conn, name='user_top',if_exists='append',index=False)\n","    hover_users.to_sql(con=conn, name='user_hover',if_exists='append',index=False)\n","    print(f''' Written to DB: user_agg(db): {df_user_agg.shape}, device_agg(db):{df_device_agg.shape}, top_user(db):{top_users.shape}, hover_users(db):{hover_users.shape}''')\n","    conn.commit()\n","#-------------------------------End of User class ---------------------------------------\n","'''\n","  Class to load and save 'Insurance' data\n","'''\n","class Insurance(Base):\n","\n","  def __init__(self, data_dir=\"/content/data\"):\n","    Base.__init__(\n","        self,\n","        'insurance',\n","         ['year','quarter', 'geo_type', 'geo_name', 'geo_pname','category', 'stat_type', 'count', 'amount'],\n","        data_dir,\n","        top_columns=['year','quarter', 'geo_type', 'geo_name','geo_pname', 'top_in','stat_type', 'count', 'amount'],\n","        hover_columns=['year','quarter', 'geo_type', 'geo_name','geo_pname', 'hover_over','stat_type', 'count', 'amount'])\n","\n","  '''\n","    Decodes aggregate insurance data\n","  '''\n","  def __decode_aggregate__(self,f,result):\n","     #print('decode_ins - Start')\n","\n","     # Load JSON contents into a temp dataframe\n","      df=Base.__load_file__(f)\n","\n","      # print(df.to_markdown())\n","      # Seek to the contents of the 'insurance' data\n","      ins_recs = df.loc['transactionData','data']\n","\n","      # For each 'insurance' row populate fields\n","      for ins_rec in ins_recs:\n","        # Collect the common fields from path name\n","        Base.__decode_common__(f, result)\n","        for payment_rec in ins_rec['paymentInstruments']:\n","          result['category'].append(ins_rec[\"name\"])\n","          result['stat_type'].append(payment_rec[\"type\"])\n","          result['count'].append(payment_rec[\"count\"])\n","          result['amount'].append(payment_rec[\"amount\"])\n","      #print('decode_ins - End')\n","\n","  '''\n","    Decodes top insurance data\n","  '''\n","  def __decode_top__(self,f,result):\n","    # Load JSON contents into a temp dataframe\n","    df=Base.__load_file__(f)\n","\n","    # Seek to the contents of the 'transaction' data\n","    ins_states = df.loc['states','data']\n","    ins_districts = df.loc['districts','data']\n","    ins_pincodes = df.loc['pincodes','data']\n","\n","    if ins_states is not None:\n","      for ins in ins_states:\n","        Base.__decode_common__(f,result)\n","        result['top_in'].append(result['geo_type'][-1])\n","        result['geo_type'][-1]='STA'\n","        result['geo_pname'][-1]='india'\n","        result['geo_name'][-1]=ins['entityName']\n","        result['stat_type'].append(ins['metric'][\"type\"])\n","        result['count'].append(ins['metric'][\"count\"])\n","        result['amount'].append(ins['metric'][\"amount\"])\n","\n","    if ins_districts is not None:\n","      for ins in ins_districts:\n","        Base.__decode_common__(f,result)\n","        result['top_in'].append(result['geo_type'][-1])\n","        result['geo_type'][-1]='DIS'\n","        if result['top_in'][-1] == 'STA':\n","          result['geo_pname'][-1]=result['geo_name'][-1]\n","        result['geo_name'][-1]=ins['entityName']\n","        result['stat_type'].append(ins['metric'][\"type\"])\n","        result['count'].append(ins['metric'][\"count\"])\n","        result['amount'].append(ins['metric'][\"amount\"])\n","\n","    if ins_pincodes is not None:\n","      for ins in ins_pincodes:\n","        Base.__decode_common__(f,result)\n","        result['top_in'].append(result['geo_type'][-1])\n","        result['geo_type'][-1]='PIN'\n","        if result['top_in'][-1] == 'STA':\n","          result['geo_pname'][-1]=result['geo_name'][-1]\n","        result['geo_name'][-1]=ins['entityName']\n","        result['stat_type'].append(ins['metric'][\"type\"])\n","        result['count'].append(ins['metric'][\"count\"])\n","        result['amount'].append(ins['metric'][\"amount\"])\n","\n","  def __decode_hover__(self,f,result):\n","    # Load JSON contents into a temp dataframe\n","    df=Base.__load_file__(f)\n","    #print(df.to_markdown())\n","    txn_hover_recs = df.loc['hoverDataList','data']\n","    for txn_hover_rec in txn_hover_recs:\n","      # Collect the common fields from path name\n","      Base.__decode_common__(f,result)\n","      result['hover_over'].append(result['geo_type'][-1])\n","      result['geo_pname'][-1]=result['geo_name'][-1]\n","\n","      if result['hover_over'][-1] == 'CON':\n","        result['geo_type'][-1]='STA'\n","      else:\n","        result['geo_type'][-1]='DIS'\n","\n","      metric=txn_hover_rec['metric'][0]\n","      result['geo_name'][-1]=txn_hover_rec['name']\n","      if result['geo_type'][-1] == 'DIS':\n","        result['geo_name'][-1] = result['geo_name'][-1].split(' district')[0] # Remove  the ' district' at the end of the name\n","\n","      result['stat_type'].append(metric['type'])\n","      result['count'].append(metric['count'])\n","      result['amount'].append(metric['amount'])\n","\n","  def __preprocess_and_store__(self, conn, geo):\n","    agg_ins = self.__preprocess__(self.__aggregated_result__, geo=geo)\n","    top_ins = self.__preprocess__(self.__top_result__, geo=geo)\n","    hover_ins = self.__preprocess__(self.__hover_result__, geo=geo)\n","\n","    print(f'''agg_ins(db): {agg_ins.shape}, top_ins(db):{top_ins.shape}, , hover_ins(db):{hover_ins.shape}''')\n","\n","    # Write to 'transaction_*' tables\n","    agg_ins.to_sql(con=conn, name='insurance_agg',if_exists='append',index=False)\n","    top_ins.to_sql(con=conn, name='insurance_top',if_exists='append',index=False)\n","    hover_ins.to_sql(con=conn, name='insurance_hover',if_exists='append',index=False)\n","\n","    conn.commit()\n","#------------------------------------------------ End Insurance class -------------------------------\n","\n","class Geo:\n","  def __init__(self,data_dir='/content/data'):\n","    self.__data_dir__= data_dir\n","    self.__states__=dict()\n","    self.__districts__=dict()\n","    self.__pincodes__=dict()\n","    self.__preprocessed_states__=pd.DataFrame({})\n","  def index(self):\n","    for f in __get_abs_fpaths__(self.__data_dir__):\n","      state=None\n","      if 'state' in f:\n","        state= os.path.normpath(f).split(os.sep)[-3]\n","        # Index the state\n","        if state not in self.__states__:\n","          self.__states__[state]='india'\n","\n","        if 'top' in f:\n","          # Load JSON contents into a temp dataframe\n","          df=Base.__load_file__(f)\n","\n","          # Seek to the contents of the 'transaction' data\n","          districts = df.loc['districts','data']\n","          pincodes = df.loc['pincodes','data']\n","\n","          # Index the districts in the file\n","          if districts is not None:\n","            for entry in districts:\n","              if 'entityName' in entry:\n","                district = entry['entityName']\n","              elif 'name' in entry:\n","                district = entry['name']\n","              if district not in self.__districts__:\n","                self.__districts__[district] = state\n","              else:\n","                if self.__districts__[district] != state:\n","                  self.__districts__[district+\"|\"+state]=state\n","\n","          # Index the pincodes in the file\n","          if pincodes is not None:\n","            for entry in pincodes:\n","              if 'entityName' in entry:\n","                pincode = entry['entityName']\n","              elif 'name' in entry:\n","                pincode = entry['name']\n","              if pincode not in self.__pincodes__:\n","                self.__pincodes__[pincode] = state\n","\n","        elif 'hover' in f:\n","          df=Base.__load_file__(f)\n","\n","          # Seek to the contents of the 'transaction' data\n","          if 'user' in f:\n","            recs = df.loc['hoverData','data']\n","            for rec_key in recs.keys():\n","              district=rec_key\n","              district=district.split(' district')[0]\n","              if district not in self.__districts__:\n","                self.__districts__[district] = state\n","              else:\n","                if self.__districts__[district] != state:\n","                  self.__districts__[district+\"|\"+state]=state\n","          else:\n","            recs = df.loc['hoverDataList','data']\n","            for rec in recs:\n","              district=rec['name']\n","              district=district.split(' district')[0]\n","              if district not in self.__districts__:\n","                self.__districts__[district] = state\n","              else:\n","                if self.__districts__[district] != state:\n","                  self.__districts__[district+\"|\"+state]=state\n","\n","\n","  def states(self):\n","    df = pd.DataFrame.from_dict(self.__states__,orient='index').reset_index().rename(columns={'index':'name', 0:'parent'})\n","    df.name = df.name.astype(str)\n","    df.parent = df.parent.astype(str)\n","    return df\n","\n","  def districts(self):\n","    df=pd.DataFrame.from_dict(self.__districts__,orient='index').reset_index().rename(columns={'index':'name', 0:'parent'})\n","    df.name = df.name.astype(str)\n","    df.parent = df.parent.astype(str)\n","    return df\n","\n","  def pincodes(self):\n","    df= pd.DataFrame.from_dict(self.__pincodes__,orient='index').reset_index().rename(columns={'index':'name', 0:'parent'})\n","    df.name = df.name.astype(str)\n","    df.parent = df.parent.astype(str)\n","    return df\n","\n","  def to_csv(self):\n","    if self.__states__  is not None:\n","      self.states().to_csv('states.csv', sep=',', encoding='utf-8')\n","\n","    if self.__districts__  is not None:\n","      self.districts().to_csv('districts.csv', sep=',', encoding='utf-8')\n","\n","    if self.__pincodes__  is not None:\n","      self.pincodes().to_csv('pincodes.csv', sep=',', encoding='utf-8')\n","\n","  def from_csv(self,folder):\n","    f=f'''{folder}/states.csv'''\n","    if os.path.exists(f):\n","      df=pd.read_csv(f).set_index(keys=['name'],drop=True)\n","      self.__states__ = df.to_dict(orient='dict', index=True)['parent']\n","\n","    f=f'''{folder}/districts.csv'''\n","    if os.path.exists(f):\n","      df=pd.read_csv(f).set_index(keys=['name'],drop=True)\n","      self.__districts__ = df.to_dict(orient='dict', index=True)['parent']\n","\n","    f=f'''{folder}/pincodes.csv'''\n","    if os.path.exists(f):\n","      df=pd.read_csv(f,dtype={'name':str,'parent':str}).set_index(keys=['name'],drop=True)\n","      self.__pincodes__ = df.to_dict(orient='dict', index=True)['parent']\n","\n","  def lookup_state_id(state_name, states):\n","    ret=None\n","    ret = states[states['name']==state_name].reset_index()['id'][0]\n","    return ret;\n","\n","  def __preprocess_and_store__(self, conn):\n","    pincodes=self.pincodes() #.convert_dtypes()\n","    states = self.states()\n","    districts = self.districts()\n","\n","    '''\n","      Prepare the 'states' DF for DB insertion\n","    '''\n","    # Add geo_type column and set the value to 'STA'\n","    states['geo_type']='STA'\n","\n","    # Replace the DF column names to match the DB column names\n","    states=states.reset_index().rename(columns={'index': 'id'} )\n","\n","    # Assign non-conflicting IDs for DB rows\n","    states.id=states.id+1\n","\n","    '''\n","      Prepare the 'districts' DF for DB insertion\n","    '''\n","    # Add geo_type column and set the value to 'DIS'\n","    districts['geo_type']='DIS'\n","    #Cleanup state name\n","    districts['name']=districts['name'].apply(lambda x: x.split('|')[0] )\n","\n","    # Replace the DF column names to match the DB column names\n","    districts=districts.reset_index().rename(columns={'index': 'id'})\n","\n","    # Assign non-conflicting DB ids for the districts\n","    districts.id=districts.id+len(states.index)+1\n","\n","    '''\n","      Prepare the 'pincodes' DF for DB insertion\n","    '''\n","    # Add geo_type column and set the value to 'PIN'\n","    pincodes['geo_type']='PIN'\n","\n","    # Replace the DF column names to match the DB column names\n","    pincodes=pincodes.reset_index().rename(columns={'index': 'id'} )\n","\n","    # Assign non-conflicting DB ids for the districts\n","    pincodes.id=pincodes.id+len(states.index)+ len(districts.index)+1\n","\n","    # Impute null values (NOTE: Analyis showed that - th  e only NA value corrsponded to 194108 for parent state ID=18)\n","    pincodes['name']=pincodes.name.fillna('194108')\n","\n","    '''\n","      Replace parent names with IDs assigned in previous steps\n","    '''\n","    states['parent_id']=0\n","\n","    districts['parent_id']=districts['parent'].apply(Geo.lookup_state_id,args=(states,))\n","    pincodes['parent_id']=pincodes['parent'].apply(Geo.lookup_state_id,args=(states,))\n","\n","    states.drop(columns=['parent'], inplace=True)\n","    districts.drop(columns=['parent'], inplace=True)\n","    pincodes.drop(columns=['parent'], inplace=True)\n","\n","    # Write to the DB\n","    country=pd.DataFrame.from_dict({'id':0, 'name':'india', 'geo_type':'CON'}, orient='index').T\n","    country.to_sql(con=conn, name='geo',if_exists='append',index=False)\n","    states.to_sql(con=conn, name='geo',if_exists='append',index=False)\n","    districts.to_sql(con=conn, name='geo',if_exists='append',index=False)\n","    pincodes.to_sql(con=conn, name='geo',if_exists='append',index=False)\n","\n","  def store(self,conn):\n","    self.__preprocess_and_store__(conn)\n","\n","  def df_from_db(self, conn):\n","    sql = '''SELECT id,name,geo_type,parent_id FROM geo;'''\n","    return pd.read_sql_query(sql, conn)\n","\n","\n"]},{"cell_type":"code","execution_count":89,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10418,"status":"ok","timestamp":1723625806616,"user":{"displayName":"Hariharan Ganapathy","userId":"18052769655460982476"},"user_tz":-330},"id":"vKcu-IZFLiVq","outputId":"a93cbda5-07aa-45a8-957b-55af3a5cbd48"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Updated property [core/project].\n"]}],"source":["'''\n","  Configure the Test Environment\n","'''\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","# Set the directory variables\n","data_dir = \"/content/drive/MyDrive/Learn/guvi/labs/Assignments/PhonePePulse/phonepe/pulse/data\"\n","csv_dir='/content/'\n","\n","from google.colab import auth\n","\n","# Aunthenticate to Google Colab\n","auth.authenticate_user()\n","\n","#Configure Google Cloud\n","from tools import load_db_cfg\n","dbcfg=load_db_cfg('pp.json')\n","!gcloud config set project {dbcfg['project_id']}\n","\n","'''\n","  Enable Google SQL Admin API Services\n","'''\n","# Enable Cloud SQL Admin API\n","!gcloud services enable sqladmin.googleapis.com\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2238978,"status":"ok","timestamp":1723024098935,"user":{"displayName":"Hariharan Ganapathy","userId":"18052769655460982476"},"user_tz":-330},"id":"9vpQ9BrDelaX","outputId":"179f4cd6-cec0-4ca7-e253-3812b1185448"},"outputs":[{"output_type":"stream","name":"stdout","text":["Indexing States, Districts and Pincodes from  gdrive-github clone.. This may take several minutes...!!!\n","Geo: states=(36, 2), districts=(733, 2), pincodes=(1115, 2)\n"]}],"source":["'''\n","  Test - Index States,Districts and Pincodes\n","'''\n","print(f'''Indexing States, Districts and Pincodes from  gdrive-github clone.. This may take several minutes...!!!''')\n","g=Geo(data_dir)\n","g.index()\n","print (f'Geo: states={g.states().shape}, districts={g.districts().shape}, pincodes={g.pincodes().shape}')\n","g.to_csv()\n"]},{"cell_type":"code","execution_count":90,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":740,"status":"ok","timestamp":1723625819141,"user":{"displayName":"Hariharan Ganapathy","userId":"18052769655460982476"},"user_tz":-330},"id":"5ICQOrRMwm3r","outputId":"a17ee09f-c299-4372-d2c0-1eb2c33a40ec"},"outputs":[{"output_type":"stream","name":"stdout","text":["Geo: states=(36, 2), districts=(733, 2), pincodes=(1115, 2)\n","Any Empty Values? states=False, districts=False, pincodes=False\n"]}],"source":["'''\n","  Test - From CSV files load 'Geo' data - States, Districts and Pincodes\n","'''\n","geo=Geo()\n","geo.from_csv('/content')\n","states=geo.states()\n","districts=geo.districts()\n","pincodes=geo.pincodes()\n","print (f'Geo: states={states.shape}, districts={districts.shape}, pincodes={pincodes.shape}')\n","print(f'Any Empty Values? states={states.isna().any().any()}, districts={districts.isna().any().any()}, pincodes={pincodes.isna().any().any()}')"]},{"cell_type":"code","execution_count":91,"metadata":{"id":"3i2w-BpB848q","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723625827702,"user_tz":-330,"elapsed":3549,"user":{"displayName":"Hariharan Ganapathy","userId":"18052769655460982476"}},"outputId":"bcf832ed-1524-47d8-cb3a-f8018f88e6dc"},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: cloud-sql-python-connector in /usr/local/lib/python3.10/dist-packages (1.12.0)\n","Requirement already satisfied: aiofiles in /usr/local/lib/python3.10/dist-packages (from cloud-sql-python-connector) (24.1.0)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from cloud-sql-python-connector) (3.10.1)\n","Requirement already satisfied: cryptography>=42.0.0 in /usr/local/lib/python3.10/dist-packages (from cloud-sql-python-connector) (42.0.8)\n","Requirement already satisfied: Requests in /usr/local/lib/python3.10/dist-packages (from cloud-sql-python-connector) (2.32.3)\n","Requirement already satisfied: google-auth>=2.28.0 in /usr/local/lib/python3.10/dist-packages (from cloud-sql-python-connector) (2.33.0)\n","Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.10/dist-packages (from cryptography>=42.0.0->cloud-sql-python-connector) (1.17.0)\n","Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.28.0->cloud-sql-python-connector) (5.4.0)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.28.0->cloud-sql-python-connector) (0.4.0)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth>=2.28.0->cloud-sql-python-connector) (4.9)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->cloud-sql-python-connector) (2.3.4)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->cloud-sql-python-connector) (1.3.1)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->cloud-sql-python-connector) (24.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->cloud-sql-python-connector) (1.4.1)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->cloud-sql-python-connector) (6.0.5)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->cloud-sql-python-connector) (1.9.4)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->cloud-sql-python-connector) (4.0.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from Requests->cloud-sql-python-connector) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from Requests->cloud-sql-python-connector) (3.7)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from Requests->cloud-sql-python-connector) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from Requests->cloud-sql-python-connector) (2024.7.4)\n","Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.12->cryptography>=42.0.0->cloud-sql-python-connector) (2.22)\n","Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth>=2.28.0->cloud-sql-python-connector) (0.6.0)\n"]}],"source":["!pip install cloud-sql-python-connector"]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7958,"status":"ok","timestamp":1723610724748,"user":{"displayName":"Hariharan Ganapathy","userId":"18052769655460982476"},"user_tz":-330},"id":"XF8vAJlf9Y-f","outputId":"39ac0df8-b5ec-4f17-9afb-71b2a8ebc7a4"},"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting pymysql\n","  Downloading PyMySQL-1.1.1-py3-none-any.whl.metadata (4.4 kB)\n","Downloading PyMySQL-1.1.1-py3-none-any.whl (44 kB)\n","\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/45.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.0/45.0 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: pymysql\n","Successfully installed pymysql-1.1.1\n"]}],"source":["# Install PyMySQL python module\n","!pip install pymysql"]},{"cell_type":"code","execution_count":92,"metadata":{"id":"LILrtxB-fN1z","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1723625845567,"user_tz":-330,"elapsed":7171,"user":{"displayName":"Hariharan Ganapathy","userId":"18052769655460982476"}},"outputId":"4b65c826-420d-436f-c831-005570e1133f"},"outputs":[{"output_type":"stream","name":"stdout","text":["Successfully connected to 'phonepe_pulse' database!! \n"]},{"output_type":"stream","name":"stderr","text":["ERROR:sqlalchemy.pool.impl.QueuePool:Exception during reset or similar\n","Traceback (most recent call last):\n","  File \"/usr/local/lib/python3.10/dist-packages/sqlalchemy/pool/base.py\", line 986, in _finalize_fairy\n","    fairy._reset(\n","  File \"/usr/local/lib/python3.10/dist-packages/sqlalchemy/pool/base.py\", line 1432, in _reset\n","    pool._dialect.do_rollback(self)\n","  File \"/usr/local/lib/python3.10/dist-packages/sqlalchemy/engine/default.py\", line 699, in do_rollback\n","    dbapi_connection.rollback()\n","  File \"/usr/local/lib/python3.10/dist-packages/pymysql/connections.py\", line 493, in rollback\n","    self._read_ok_packet()\n","  File \"/usr/local/lib/python3.10/dist-packages/pymysql/connections.py\", line 453, in _read_ok_packet\n","    pkt = self._read_packet()\n","  File \"/usr/local/lib/python3.10/dist-packages/pymysql/connections.py\", line 744, in _read_packet\n","    packet_header = self._read_bytes(4)\n","  File \"/usr/local/lib/python3.10/dist-packages/pymysql/connections.py\", line 798, in _read_bytes\n","    raise err.OperationalError(\n","pymysql.err.OperationalError: (2013, 'Lost connection to MySQL server during query')\n"]}],"source":["'''\n","Connect to DB\n","'''\n","from tools import load_db_cfg\n","from dbconnect import DbConnector\n","dbcfg=load_db_cfg('pp.json')\n","pool = DbConnector(dbcfg)\n","con=pool.connect()\n","\n","geo=Geo()\n","geo.from_csv('/content')\n","geo.store(con)\n","con.close()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":396059,"status":"ok","timestamp":1723025221064,"user":{"displayName":"Hariharan Ganapathy","userId":"18052769655460982476"},"user_tz":-330},"id":"BocCrTVqEOx7","outputId":"ec7a6fbc-8066-4e86-8f1a-0845e26a74bf"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading transactions from remote gdrive-github clone.. This may take several minutes...!!!\n","Transactions: txn_agg=(4619, 9), txn_top=(17074, 9), txn_hover=(19196, 9)\n","Empty Values: txn_agg=True, txn_top=True, txn_hover=False\n"]}],"source":["'''\n","  Test - Load 'Transaction' data from gdrive-github clone\n","  Test - Save in local CSV\n","'''\n","print(f'''Loading transactions from remote gdrive-github clone.. This may take several minutes...!!!''')\n","# Load aggregated, top, hover transactions\n","t=Transaction(data_dir)\n","t.load_aggregated()\n","t.load_top()\n","t.load_hover()\n","\n","# Verify the dataframe\n","print(f'''Transactions: txn_agg={t.aggregated().shape}, txn_top={t.top().shape}, txn_hover={t.hover().shape}''')\n","print(f'Empty Values: txn_agg={t.aggregated().isna().any().any()}, txn_top={t.top().isna().any().any()}, txn_hover={t.hover().isna().any().any()}')\n","\n","#Write the loaded transactions to CSVs\n","t.to_csv()\n"]},{"cell_type":"code","execution_count":93,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":496,"status":"ok","timestamp":1723625866614,"user":{"displayName":"Hariharan Ganapathy","userId":"18052769655460982476"},"user_tz":-330},"id":"Bn8wu_vLY_TB","outputId":"b52f56be-2a23-4e03-de15-13501ce6f703"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading transactions from local CSV folder csv_dir...!!!\n","Transactions (from CSV): txn_agg=(4619, 9), txn_top=(17074, 9), txn_hover=(19196, 9)\n","Empty Values: txn_agg=True, txn_top=True, txn_hover=False\n"]}],"source":["'''\n","  Test - From CSV files load 'Transaction' data\n","'''\n","print(f'''Loading transactions from local CSV folder {'csv_dir'}...!!!''')\n","t1=Transaction()\n","t1.from_csv(csv_dir)\n","\n","print(f'''Transactions (from CSV): txn_agg={t1.aggregated().shape}, txn_top={t1.top().shape}, txn_hover={t1.hover().shape}''')\n","print(f'Empty Values: txn_agg={t1.aggregated().isna().any().any()}, txn_top={t1.top().isna().any().any()}, txn_hover={t1.hover().isna().any().any()}')\n"]},{"cell_type":"code","execution_count":94,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":71971,"status":"ok","timestamp":1723625942952,"user":{"displayName":"Hariharan Ganapathy","userId":"18052769655460982476"},"user_tz":-330},"id":"9UZCi3mmYBR5","outputId":"074f332f-c2ab-44bd-bb2c-d44b60bbf58d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Successfully connected to 'phonepe_pulse' database!! \n","agg_txns(db): (4619, 8), top_txns(db):(17074, 8), hover_txns(db):(19196, 8)\n"]}],"source":["'''\n","  Test - Store 'Transaction' data in DB\n","'''\n","from tools import load_db_cfg\n","from dbconnect import DbConnector\n","dbcfg=load_db_cfg('pp.json')\n","pool = DbConnector(dbcfg)\n","con=pool.connect()\n","\n","geo=Geo()\n","geo.from_csv('/content')\n","\n","t1.store(con,geo.df_from_db(con))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18644,"status":"ok","timestamp":1722971166854,"user":{"displayName":"Hariharan Ganapathy","userId":"18052769655460982476"},"user_tz":-330},"id":"8CMObs3YfrFq","outputId":"fa4fb2ca-659a-44a5-a0aa-edeaa81e2a78"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading Insurance from remote gdrive-github clone.. This may take several minutes...!!!\n","ins_agg=(590, 9), ins_top=(10792, 9), ins_hover=(12133, 9)\n"]}],"source":["'''\n","  Test - Load 'Insurance' data from gdrive-github clone\n","  Test - Save in local CSV\n","'''\n","print(f'''Loading Insurance from remote gdrive-github clone.. This may take several minutes...!!!''')\n","# Load aggregated, to, hover transactions\n","ins=Insurance(data_dir)\n","ins.load_aggregated()\n","ins.load_top()\n","ins.load_hover()\n","\n","# Verify the dataframe\n","print(f'''ins_agg={ins.aggregated().shape}, ins_top={ins.top().shape}, ins_hover={ins.hover().shape}''')\n","#print(f'''Hover Insurance: {ins.hover().shape}''')\n","\n","\n","#Write the loaded transactions to CSVs\n","ins.to_csv()"]},{"cell_type":"code","execution_count":95,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":725,"status":"ok","timestamp":1723625952480,"user":{"displayName":"Hariharan Ganapathy","userId":"18052769655460982476"},"user_tz":-330},"id":"ye_sj1elh1Zr","outputId":"85e58a26-d32a-466c-e0e3-35801c455c64"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading insurance data from local CSV folder /content/...!!!\n","ins_agg(csv)=(590, 9), ins_top(csv)=(10792, 9), ins_hover(csv)=(12133, 9)\n"]}],"source":["'''\n","  Test - From CSV files, load 'Insurance' data\n","'''\n","print(f'''Loading insurance data from local CSV folder {csv_dir}...!!!''')\n","ins1=Insurance(data_dir)\n","ins1.from_csv(csv_dir)\n","\n","print(f'''ins_agg(csv)={ins1.aggregated().shape}, ins_top(csv)={ins1.top().shape}, ins_hover(csv)={ins1.hover().shape}''')\n","#print(f'''Hover Insurance (from CSV): {ins1.hover().shape}''')\n"]},{"cell_type":"code","execution_count":96,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":43606,"status":"ok","timestamp":1723626000460,"user":{"displayName":"Hariharan Ganapathy","userId":"18052769655460982476"},"user_tz":-330},"id":"cSOkciy34Cy6","outputId":"5dac1bbb-2557-4297-cf40-4b8d4eab927b"},"outputs":[{"output_type":"stream","name":"stdout","text":["Successfully connected to 'phonepe_pulse' database!! \n","agg_ins(db): (590, 8), top_ins(db):(10792, 8), , hover_ins(db):(12133, 8)\n"]}],"source":["'''\n","  Save insurance data in db\n","'''\n","from tools import load_db_cfg\n","from dbconnect import DbConnector\n","dbcfg=load_db_cfg('pp.json')\n","pool = DbConnector(dbcfg)\n","con=pool.connect()\n","\n","geo=Geo()\n","geo.from_csv('/content')\n","\n","ins1.store(con,geo.df_from_db(con))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":436118,"status":"ok","timestamp":1722969867088,"user":{"displayName":"Hariharan Ganapathy","userId":"18052769655460982476"},"user_tz":-330},"id":"uDawnZtsi70W","outputId":"5008d519-7540-4cc4-f129-36959371aec9"},"outputs":[{"output_type":"stream","name":"stdout","text":["Remote loading of 'User' data from gdrive-github clone.. This may take several minutes...!!!\n","users_agg=(7215, 10), users_top=(17075, 7), , users_hover=(19200, 8)\n"]}],"source":["'''\n","  Test - Load 'User' data from gdrive-github clone\n","  Test - Save in local CSV\n","'''\n","print(f'''Remote loading of 'User' data from gdrive-github clone.. This may take several minutes...!!!''')\n","# Load aggregated, to, hover users\n","usr=User(data_dir)\n","usr.load_aggregated()\n","usr.load_top()\n","usr.load_hover()\n","\n","\n","# Verify the dataframe\n","print(f'''users_agg={usr.aggregated().shape}, users_top={usr.top().shape}, , users_hover={usr.hover().shape}''')\n","#print(f'''Hover Users: {usr.hover().shape}''')\n","\n","\n","#Write the loaded transactions to CSVs\n","usr.to_csv()"]},{"cell_type":"code","execution_count":97,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":642,"status":"ok","timestamp":1723626019023,"user":{"displayName":"Hariharan Ganapathy","userId":"18052769655460982476"},"user_tz":-330},"id":"A3keWF3kjXmL","outputId":"7752ffb6-03b7-4442-8b95-da1561bf3854"},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading 'User' data from local CSV folder /content/...!!!\n","users_agg(csv)=(7215, 10), users_top(csv)=(17075, 7), users_hover(csv)=(19200, 8)\n"]}],"source":["'''\n","  Test - From CSV files, load 'User' data\n","'''\n","print(f'''Loading 'User' data from local CSV folder {csv_dir}...!!!''')\n","usr1=User(data_dir)\n","usr1.from_csv(csv_dir)\n","\n","print(f'''users_agg(csv)={usr1.aggregated().shape}, users_top(csv)={usr1.top().shape}, users_hover(csv)={usr1.hover().shape}''')\n","#print(f'''Hover Users (from CSV): {usr1.hover().shape}''')"]},{"cell_type":"code","execution_count":98,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Gf3Vb59L1xrb","outputId":"acfc059e-d09a-4402-e424-a676512221f8","executionInfo":{"status":"ok","timestamp":1723626106362,"user_tz":-330,"elapsed":76429,"user":{"displayName":"Hariharan Ganapathy","userId":"18052769655460982476"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Successfully connected to 'phonepe_pulse' database!! \n","Read From CSV: agg_users(db): (7215, 9), top_users(db):(17075, 6), hover_users(db):(19200, 7)\n"]},{"output_type":"stream","name":"stderr","text":["<ipython-input-88-7d6a7ef6e2e8>:472: SettingWithCopyWarning: \n","A value is trying to be set on a copy of a slice from a DataFrame\n","\n","See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n","  df_user_agg.drop_duplicates(inplace=True,ignore_index=True)\n"]},{"output_type":"stream","name":"stdout","text":[" Written to DB: user_agg(db): (925, 7), device_agg(db):(7215, 7), top_user(db):(17075, 7), hover_users(db):(19200, 8)\n"]}],"source":["'''\n","  Save user, device info in DB\n","'''\n","from tools import load_db_cfg\n","from dbconnect import DbConnector\n","dbcfg=load_db_cfg('pp.json')\n","pool = DbConnector(dbcfg)\n","con=pool.connect()\n","\n","geo=Geo()\n","#geo.from_csv('/content')\n","\n","usr1.store(con,geo.df_from_db(con))\n","con.close()"]},{"cell_type":"code","execution_count":99,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3338,"status":"ok","timestamp":1723626119542,"user":{"displayName":"Hariharan Ganapathy","userId":"18052769655460982476"},"user_tz":-330},"id":"NB1TRQ0yYtzk","outputId":"93aa1665-07ef-494c-a192-e1a2a428e5da"},"outputs":[{"output_type":"stream","name":"stdout","text":["Successfully connected to 'phonepe_pulse' database!! \n"]},{"output_type":"execute_result","data":{"text/plain":["478"]},"metadata":{},"execution_count":99}],"source":["'''Geo lookup Test'''\n","\n","from tools import load_db_cfg\n","from dbconnect import DbConnector\n","dbcfg=load_db_cfg('pp.json')\n","pool = DbConnector(dbcfg)\n","con=pool.connect()\n","\n","geo=Geo()\n","\n","\n","Base.__lookup_geo_id__('DIS,kiphire,nagaland', geo.df_from_db(con))"]}],"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMQXrfNasku9ZuNM8gxU79e"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}